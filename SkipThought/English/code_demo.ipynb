{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Text Summarization for Legal Texts\n",
    "\n",
    "## 1. Dataset\n",
    "For this project we are concentrating on both French and English Datasets. We are using AUSTLII Dataset in this example which contains legal court procedings from Australia. The dataset contains a list of sentences and their catchphrases. We are using these catch phrases for evaluating our summarization.\n",
    "\n",
    "## 1.1 Extracting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning the dataset\n",
    "The XML files contain some parsing issues and are in UTF-8. For us to use the dataset we need to the clean the XML files we get at input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(filename):\n",
    "\t\"\"\"\n",
    "\tCleans the xml for processing\n",
    "\t1. The attribute error in catchphrase tag\n",
    "\t2. Escape characters\n",
    "\t:param filename: file path of file to clean\n",
    "\t:return: cleaned file contents\n",
    "\t\"\"\"\n",
    "\twith open(filename) as fp:\n",
    "\t\tfile_content = fp.read()\n",
    "\tfile_content = re.sub(r'<catchphrase \"id=([a-z][0-9]*)\">', r'<catchphrase id=\"\\1\">', file_content)\n",
    "\tfile_content = re.sub(r'&([a-zA-Z])([a-zA-Z]*);', r'\\1', file_content)\n",
    "\treturn file_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parsing a legal document\n",
    "Takes out all the sentences and catch phrases from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(filename):\n",
    "\t\"\"\"\n",
    "\tFetches all the sentences and catch phrases of a file and stores in a (<full text>, <catch-phrase>) tuple\n",
    "\t:param filename: file path to extract from\n",
    "\t:return: (<full text>, <catch-phrases>) for the path\n",
    "\t\"\"\"\n",
    "\tfile_content = clean(filename)\n",
    "\troot = ET.fromstring(file_content)\n",
    "\tcatchphrase_subtree = root.find('catchphrases')\n",
    "\tcatchphrases = []\n",
    "\tfor catchphrase in catchphrase_subtree:\n",
    "\t\tcatchphrases.append(catchphrase.text)\n",
    "\tsentence_subtree = root.find('sentences')\n",
    "\tfull_text = []\n",
    "\tfor sentence in sentence_subtree[:-1]:\n",
    "\t\tfull_text.append(sentence.text)\n",
    "\treturn full_text, catchphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Parsing all the documents in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruth():\n",
    "\t\"\"\"\n",
    "\tFetches all the full texts and their catch phrases and stores in a (<full text>, <catch-phrase>) tuple\n",
    "\t:return: list of (<full text>, <catch-phrase>) tuples\n",
    "\t\"\"\"\n",
    "\tprint(\"Fetching dataset...\", end=\" \")\n",
    "\tgt = []\n",
    "\tfor _, _, files in os.walk(\"./fulltext/\"):\n",
    "\t\tfor filename in files:\n",
    "\t\t\tgt.append(extract(\"./fulltext/{}\".format(filename)))\n",
    "\tprint(\"Done\")\n",
    "\treturn gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Example Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dataset... Done\n",
      "['application for leave to appeal', 'authorisation of multiple infringements of copyright established', 'prior sale of realty of one respondent to primary proceedings', 'payment of substantial part of proceeds of sale to offshore company in purported repayment of loan', 'absence of material establishing original making and purpose of loan', 'mareva and ancillary orders made by primary judge', 'affidavits disclosing assets sworn', 'orders made requiring filing of further affidavits of disclosure and cross-examination of one respondent to primary proceedings on her disclosure affidavit', 'no error in making further ancillary orders', 'leave refused', 'practice and procedure']\n"
     ]
    }
   ],
   "source": [
    "gt = getGroundTruth()\n",
    "print(gt[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Proposed Approach\n",
    "For this code demonstration we will implement a text summarizer using clustering of the sentence embeddings. To select a representative sentence for each cluster we will use an extractive approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0406 19:06:42.213455 17044 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "I0406 19:07:08.742066 17044 file_utils.py:48] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge import Rouge\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoding the sentences\n",
    "Encoding the sentences to a form which we use to do further analysis on the document as a whole. To encode the sentences we are trying out three approaches to sentence encoding:\n",
    "* Skip Thought Encoding\n",
    "* Paragram Phrase Encoding\n",
    "* BERT Sentence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence):\n",
    "    return enc_model.encode(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clustering the sentence embeddings\n",
    "We now cluster the embeddings of similar sentences, to form clusters. For this we are exploring two techniques:\n",
    "* K-Means\n",
    "* DB Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(embeddings, method=\"kmeans\", minimum_samples=6):\n",
    "\tif method == \"dbscan\":\n",
    "\t\tclusters = DBSCAN(eps=0.3, min_samples=minimum_samples)\n",
    "\telse:\n",
    "\t\tkmeans = KMeans(n_clusters=minimum_samples)\n",
    "\t\tclusters = kmeans.fit(embeddings)\n",
    "\treturn cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Select Representative Cluster\n",
    "\n",
    "For selecting the representative cluster, we will be using extractive approaches.\n",
    "\n",
    "#### 2.3.1 Centroid based extraction\n",
    "Takes the closest sentence embedding to the centroid from each cluster. We also use the position of each cluster to dictate the ordering of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_representative(clusters, sentence_embeddings):\n",
    "    closest, _ = pairwise_distances_argmin_min(clusters.cluster_centers_, sentence_embeddings)\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Catch Phrase Extraction\n",
    "As the dataset is using catch phrases for evalutaion. We need another way to extract the summaries which concentrates on catchphrases. There are many known ways of keyword extraction:\n",
    " * TF-IDF\n",
    " * RAKE\n",
    " * TextRank\n",
    " \n",
    "For this approach we are experimenting with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommonPhrase(summary):\n",
    "\tresult = []\n",
    "\tmost_common_phrase = \"\"\n",
    "\tmax_freq = 1\n",
    "\tfor n in range(10, 3, -1):\n",
    "\t\tphrases = []\n",
    "\t\tfor token in ngrams(summary.split(), n):\n",
    "\t\t\tphrases.append(' '.join(token))\n",
    "\t\tphrase, freq = Counter(phrases).most_common(1)[0]\n",
    "\t\tif freq > max_freq:\n",
    "\t\t\tmax_freq = freq\n",
    "\t\t\t# result.append((phrase, n))\n",
    "\t\t\t# print(phrase)\n",
    "\t\t\tmost_common_phrase = phrase\n",
    "\t\t\tsummary = summary.replace(phrase, '')\n",
    "\treturn most_common_phrase\n",
    "\n",
    "\n",
    "def getCatchPhrase(cluster, full_text):\n",
    "\tcluster_sent = {}\n",
    "\tcatch_phrase = []\n",
    "\tsummary = []\n",
    "\tsentence_n = len(full_text)\n",
    "\tfor sentence_id in range(sentence_n):\n",
    "\t\tlabel = cluster.labels_[sentence_id]\n",
    "\t\tif label not in cluster_sent:\n",
    "\t\t\tcluster_sent[label] = []\n",
    "\t\tcluster_sent[label].append(full_text[sentence_id])\n",
    "\tfor label in cluster_sent.keys():\n",
    "\t\tsummary_label = \" \".join(cluster_sent[label])\n",
    "\t\tcatch_phrase.append(mostCommonPhrase(summary_label))\n",
    "\treturn catch_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation\n",
    "The current state of the art models using this dataset all use ROUGE-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_sum, gt_sum):\n",
    "\t\"\"\"\n",
    "\tGives rouge score\n",
    "\t:param model_sum: list of summaries returned by the model\n",
    "\t:param gt_sum: list of ground truth summary from catchphrases\n",
    "\t:return: ROUGE score\n",
    "\t\"\"\"\n",
    "\trouge = Rouge()\n",
    "\treturn rouge.get_scores(model_sum, gt_sum, avg=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\t\"\"\"\n",
    "\tExecutes the entire pipeline of the code\n",
    "\t:return: void\n",
    "\t\"\"\"\n",
    "\tgt = getGroundTruth()\n",
    "\tmodel_sum, gt_sum = [], []\n",
    "\tprint(\"Fetching encoder model...\", end=\" \")\n",
    "\tenc_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\tprint(\"Done\")\n",
    "\tfor full_text, catch_phrases in gt:\n",
    "\t\t# Embed each sentence\n",
    "\t\tsentence_embeddings = enc_model.encode(full_text)\n",
    "\n",
    "\t\t# Cluster each embedding\n",
    "\t\tcluster_n = 11\n",
    "\t\tclusters = cluster(sentence_embeddings, minimum_samples=cluster_n)\n",
    "\t\tcentroids = []\n",
    "\t\tfor idx in range(cluster_n):\n",
    "\t\t\tcentroid_id = np.where(clusters.labels_ == idx)[0]\n",
    "\t\t\tcentroids.append(np.mean(centroid_id))\n",
    "\n",
    "\t\t# Select representative cluster\n",
    "\t\tclosest, _ = pairwise_distances_argmin_min(clusters.cluster_centers_, sentence_embeddings)\n",
    "\t\tordering = sorted(range(cluster_n), key=lambda k: centroids[k])\n",
    "\t\t\n",
    "\t\tsummary = '.'.join([full_text[closest[idx]] for idx in ordering]).replace('\\n', ' ')\n",
    "\t\tmodel_sum.append(summary)\n",
    "\t\tgt_sum.append(\".\".join(catch_phrases))\n",
    "\t\tbreak\n",
    "\tprint(\"ROUGE score: {}\".format(evaluate(model_sum, gt_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
