{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Text Summarization for Legal Texts\n",
    "\n",
    "## 1. Dataset\n",
    "For this project we are concentrating on both French and English Datasets. We are using AUSTLII Dataset in this example which contains legal court procedings from Australia. The dataset contains a list of sentences and their catchphrases. We are using these catch phrases for evaluating our summarization.\n",
    "\n",
    "## 1.1 Extracting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning the dataset\n",
    "The XML files contain some parsing issues and are in UTF-8. For us to use the dataset we need to the clean the XML files we get at input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(filename):\n",
    "\t\"\"\"\n",
    "\tCleans the xml for processing\n",
    "\t1. The attribute error in catchphrase tag\n",
    "\t2. Escape characters\n",
    "\t:param filename: file path of file to clean\n",
    "\t:return: cleaned file contents\n",
    "\t\"\"\"\n",
    "\twith open(filename) as fp:\n",
    "\t\tfile_content = fp.read()\n",
    "\tfile_content = re.sub(r'<catchphrase \"id=([a-z][0-9]*)\">', r'<catchphrase id=\"\\1\">', file_content)\n",
    "\tfile_content = re.sub(r'&([a-zA-Z])([a-zA-Z]*);', r'\\1', file_content)\n",
    "\treturn file_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parsing a legal document\n",
    "Takes out all the sentences and catch phrases from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(filename):\n",
    "\t\"\"\"\n",
    "\tFetches all the sentences and catch phrases of a file and stores in a (<full text>, <catch-phrase>) tuple\n",
    "\t:param filename: file path to extract from\n",
    "\t:return: (<full text>, <catch-phrases>) for the path\n",
    "\t\"\"\"\n",
    "\tfile_content = clean(filename)\n",
    "\troot = ET.fromstring(file_content)\n",
    "\tcatchphrase_subtree = root.find('catchphrases')\n",
    "\tcatchphrases = []\n",
    "\tfor catchphrase in catchphrase_subtree:\n",
    "\t\tcatchphrases.append(catchphrase.text)\n",
    "\tsentence_subtree = root.find('sentences')\n",
    "\tfull_text = []\n",
    "\tfor sentence in sentence_subtree[:-1]:\n",
    "\t\tfull_text.append(sentence.text)\n",
    "\treturn full_text, catchphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Parsing all the documents in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruth():\n",
    "\t\"\"\"\n",
    "\tFetches all the full texts and their catch phrases and stores in a (<full text>, <catch-phrase>) tuple\n",
    "\t:return: list of (<full text>, <catch-phrase>) tuples\n",
    "\t\"\"\"\n",
    "\tprint(\"Fetching dataset...\", end=\" \")\n",
    "\tgt = []\n",
    "\tfor _, _, files in os.walk(\"./fulltext/\"):\n",
    "\t\tfor filename in files:\n",
    "\t\t\tgt.append(extract(\"./fulltext/{}\".format(filename)))\n",
    "\tprint(\"Done\")\n",
    "\treturn gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Example Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dataset... Done\n",
      "['application for leave to appeal', 'authorisation of multiple infringements of copyright established', 'prior sale of realty of one respondent to primary proceedings', 'payment of substantial part of proceeds of sale to offshore company in purported repayment of loan', 'absence of material establishing original making and purpose of loan', 'mareva and ancillary orders made by primary judge', 'affidavits disclosing assets sworn', 'orders made requiring filing of further affidavits of disclosure and cross-examination of one respondent to primary proceedings on her disclosure affidavit', 'no error in making further ancillary orders', 'leave refused', 'practice and procedure']\n"
     ]
    }
   ],
   "source": [
    "gt = getGroundTruth()\n",
    "print(gt[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Proposed Approach\n",
    "For this code demonstration we will implement a text summarizer using clustering of the sentence embeddings. To select a representative sentence for each cluster we will use an extractive approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge import Rouge\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoding the sentences\n",
    "Encoding the sentences to a form which we use to do further analysis on the document as a whole. To encode the sentences we are trying out three approaches to sentence encoding:\n",
    "* Skip Thought Encoding\n",
    "* Paragram Phrase Encoding\n",
    "* BERT Sentence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0406 19:26:28.920201 17044 SentenceTransformer.py:29] Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "I0406 19:26:28.923205 17044 SentenceTransformer.py:32] Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "I0406 19:26:28.928204 17044 SentenceTransformer.py:68] Load SentenceTransformer from folder: C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "I0406 19:26:28.934205 17044 configuration_utils.py:182] loading configuration file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\config.json\n",
      "I0406 19:26:28.938204 17044 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0406 19:26:28.941204 17044 modeling_utils.py:403] loading weights file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\pytorch_model.bin\n",
      "I0406 19:26:31.363287 17044 tokenization_utils.py:327] Model name 'C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT' is a path or url to a directory containing tokenizer files.\n",
      "I0406 19:26:31.366290 17044 tokenization_utils.py:359] Didn't find file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\tokenizer_config.json. We won't load it.\n",
      "I0406 19:26:31.368289 17044 tokenization_utils.py:395] loading file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\vocab.txt\n",
      "I0406 19:26:31.370288 17044 tokenization_utils.py:395] loading file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\added_tokens.json\n",
      "I0406 19:26:31.371289 17044 tokenization_utils.py:395] loading file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\special_tokens_map.json\n",
      "I0406 19:26:31.372289 17044 tokenization_utils.py:395] loading file None\n",
      "I0406 19:26:31.483481 17044 SentenceTransformer.py:89] Use pytorch device: cuda\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'skipthoughts' has no attribute 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-dbd221ebf063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Skip Thought\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskipthoughts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskipthoughts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'skipthoughts' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "# BERT Encoding of sentences\n",
    "enc_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Skip Thought\n",
    "model = skipthoughts.load_model()\n",
    "encoder = skipthoughts.Encoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence):\n",
    "    return enc_model.encode(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clustering the sentence embeddings\n",
    "We now cluster the embeddings of similar sentences, to form clusters. For this we are exploring two techniques:\n",
    "* K-Means\n",
    "* DB Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(embeddings, method=\"kmeans\", minimum_samples=6):\n",
    "\tif method == \"dbscan\":\n",
    "\t\tclusters = DBSCAN(eps=0.3, min_samples=minimum_samples)\n",
    "\telse:\n",
    "\t\tkmeans = KMeans(n_clusters=minimum_samples)\n",
    "\t\tclusters = kmeans.fit(embeddings)\n",
    "\treturn clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Select Representative Cluster\n",
    "\n",
    "For selecting the representative cluster, we will be using extractive approaches.\n",
    "\n",
    "#### 2.3.1 Centroid based extraction\n",
    "Takes the closest sentence embedding to the centroid from each cluster. We also use the position of each cluster to dictate the ordering of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_representative(clusters, sentence_embeddings):\n",
    "    closest, _ = pairwise_distances_argmin_min(clusters.cluster_centers_, sentence_embeddings)\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Catch Phrase Extraction\n",
    "As the dataset is using catch phrases for evalutaion. We need another way to extract the summaries which concentrates on catchphrases. There are many known ways of keyword extraction:\n",
    " * TF-IDF\n",
    " * RAKE\n",
    " * TextRank\n",
    " \n",
    "For this approach we are experimenting with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommonPhrase(summary):\n",
    "\tresult = []\n",
    "\tmost_common_phrase = \"\"\n",
    "\tmax_freq = 1\n",
    "\tfor n in range(10, 3, -1):\n",
    "\t\tphrases = []\n",
    "\t\tfor token in ngrams(summary.split(), n):\n",
    "\t\t\tphrases.append(' '.join(token))\n",
    "\t\tphrase, freq = Counter(phrases).most_common(1)[0]\n",
    "\t\tif freq > max_freq:\n",
    "\t\t\tmax_freq = freq\n",
    "\t\t\t# result.append((phrase, n))\n",
    "\t\t\t# print(phrase)\n",
    "\t\t\tmost_common_phrase = phrase\n",
    "\t\t\tsummary = summary.replace(phrase, '')\n",
    "\treturn most_common_phrase\n",
    "\n",
    "\n",
    "def getCatchPhrase(cluster, full_text):\n",
    "\tcluster_sent = {}\n",
    "\tcatch_phrase = []\n",
    "\tsummary = []\n",
    "\tsentence_n = len(full_text)\n",
    "\tfor sentence_id in range(sentence_n):\n",
    "\t\tlabel = cluster.labels_[sentence_id]\n",
    "\t\tif label not in cluster_sent:\n",
    "\t\t\tcluster_sent[label] = []\n",
    "\t\tcluster_sent[label].append(full_text[sentence_id])\n",
    "\tfor label in cluster_sent.keys():\n",
    "\t\tsummary_label = \" \".join(cluster_sent[label])\n",
    "\t\tcatch_phrase.append(mostCommonPhrase(summary_label))\n",
    "\treturn catch_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation\n",
    "The current state of the art models using this dataset all use ROUGE-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_sum, gt_sum):\n",
    "\t\"\"\"\n",
    "\tGives rouge score\n",
    "\t:param model_sum: list of summaries returned by the model\n",
    "\t:param gt_sum: list of ground truth summary from catchphrases\n",
    "\t:return: ROUGE score\n",
    "\t\"\"\"\n",
    "\trouge = Rouge()\n",
    "\treturn rouge.get_scores(model_sum, gt_sum, avg=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\t\"\"\"\n",
    "\tExecutes the entire pipeline of the code\n",
    "\t:return: void\n",
    "\t\"\"\"\n",
    "\tgt = getGroundTruth()\n",
    "\tmodel_sum, gt_sum = [], []\n",
    "\tprint(\"Fetching encoder model...\", end=\" \")\n",
    "\tenc_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\tprint(\"Done\")\n",
    "\tfor full_text, catch_phrases in gt[:15]:\n",
    "\t\t# Embed each sentence\n",
    "\t\tsentence_embeddings = enc_model.encode(full_text)\n",
    "\n",
    "\t\t# Cluster each embedding\n",
    "\t\tcluster_n = 11\n",
    "\t\tclusters = cluster(sentence_embeddings, minimum_samples=cluster_n)\n",
    "\t\tcentroids = []\n",
    "\t\tfor idx in range(cluster_n):\n",
    "\t\t\tcentroid_id = np.where(clusters.labels_ == idx)[0]\n",
    "\t\t\tcentroids.append(np.mean(centroid_id))\n",
    "\n",
    "\t\t# Select representative cluster\n",
    "\t\tclosest, _ = pairwise_distances_argmin_min(clusters.cluster_centers_, sentence_embeddings)\n",
    "\t\tordering = sorted(range(cluster_n), key=lambda k: centroids[k])\n",
    "\t\t\n",
    "\t\tsummary = '.'.join([full_text[closest[idx]] for idx in ordering]).replace('\\n', ' ')\n",
    "\t\tmodel_sum.append(summary)\n",
    "\t\tgt_sum.append(\".\".join(catch_phrases))\n",
    "\t\tbreak\n",
    "\tprint(\"ROUGE score: {}\".format(evaluate(model_sum, gt_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dataset... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0406 19:22:46.262805 17044 SentenceTransformer.py:29] Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "I0406 19:22:46.263804 17044 SentenceTransformer.py:32] Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "I0406 19:22:46.265806 17044 SentenceTransformer.py:68] Load SentenceTransformer from folder: C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "I0406 19:22:46.268806 17044 configuration_utils.py:182] loading configuration file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\config.json\n",
      "I0406 19:22:46.270806 17044 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0406 19:22:46.272805 17044 modeling_utils.py:403] loading weights file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Fetching encoder model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0406 19:22:48.291752 17044 tokenization_utils.py:327] Model name 'C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT' is a path or url to a directory containing tokenizer files.\n",
      "I0406 19:22:48.293752 17044 tokenization_utils.py:359] Didn't find file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\tokenizer_config.json. We won't load it.\n",
      "I0406 19:22:48.296756 17044 tokenization_utils.py:395] loading file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\vocab.txt\n",
      "I0406 19:22:48.297755 17044 tokenization_utils.py:395] loading file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\added_tokens.json\n",
      "I0406 19:22:48.298755 17044 tokenization_utils.py:395] loading file C:\\Users\\13235/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\special_tokens_map.json\n",
      "I0406 19:22:48.299755 17044 tokenization_utils.py:395] loading file None\n",
      "I0406 19:22:48.350756 17044 SentenceTransformer.py:89] Use pytorch device: cuda\n",
      "Batches:   7%|▋         | 2/29 [00:00<00:01, 19.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 29/29 [00:02<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE score: {'rouge-1': {'f': 0.17447495705888652, 'p': 0.10266159695817491, 'r': 0.5806451612903226}, 'rouge-2': {'f': 0.03241490832149097, 'p': 0.01904761904761905, 'r': 0.10869565217391304}, 'rouge-l': {'f': 0.13213212925538156, 'p': 0.08, 'r': 0.3793103448275862}}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Implementation using Skip Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dataset... Done\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'skipthoughts' has no attribute 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-624f02dd0ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ROUGE score: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mmain_skip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-624f02dd0ada>\u001b[0m in \u001b[0;36mmain_skip\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#print(\"Fetching encoder model...\", end=\" \")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#enc_model = SentenceTransformer('bert-base-nli-mean-tokens')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskipthoughts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskipthoughts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#print(\"Done\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'skipthoughts' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "def main_skip():\n",
    "    \"\"\"\n",
    "    Executes the entire pipeline of the code\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "    gt = getGroundTruth()\n",
    "    model_sum, gt_sum = [], []\n",
    "    #print(\"Fetching encoder model...\", end=\" \")\n",
    "    #enc_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    model = skipthoughts.load_model()\n",
    "    encoder = skipthoughts.Encoder(model)\n",
    "    #print(\"Done\")\n",
    "    for full_text, catch_phrases in gt:\n",
    "        # Embed each sentence\n",
    "        #sentence_embeddings = enc_model.encode(full_text)\n",
    "        print('File Length:',len(full_text))\n",
    "        encoded =  encoder.encode(full_text)\n",
    "        # Cluster each embedding\n",
    "        cluster_n = 11\n",
    "        #clusters = cluster(sentence_embeddings, minimum_samples=cluster_n)\n",
    "        clusters = cluster(encoded, minimum_samples=cluster_n)\n",
    "        centroids = []\n",
    "        for idx in range(cluster_n):\n",
    "            centroid_id = np.where(clusters.labels_ == idx)[0]\n",
    "            centroids.append(np.mean(centroid_id))\n",
    "\n",
    "        # Select representative cluster\n",
    "        closest, _ = pairwise_distances_argmin_min(clusters.cluster_centers_, encoded)\n",
    "        ordering = sorted(range(cluster_n), key=lambda k: centroids[k])\n",
    "        #print(ordering)\n",
    "        summary = ' '.join([full_text[closest[idx]] for idx in ordering]).replace('\\n', ' ')\n",
    "        model_sum.append(summary)\n",
    "        #print([(full_text[closest[idx]], closest[idx]) for idx in ordering])\n",
    "        print(summary)\n",
    "        #print(len(catch_phrases))\n",
    "        #print(\".\".join(catch_phrases))\n",
    "        gt_sum.append(\".\".join(catch_phrases))\n",
    "        # break\n",
    "    print(\"ROUGE score: {}\".format(evaluate(model_sum, gt_sum)))\n",
    "main_skip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
